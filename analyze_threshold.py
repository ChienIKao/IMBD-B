# åˆ†ææ¯å€‹æ¸¬è©¦æª”æ¡ˆçš„è©³ç´°ç•°å¸¸æ¯”ä¾‹

import os
import torch
import numpy as np
import argparse
import joblib
from src import data_loader, model

def analyze_all_test_files(model_dir="./models"):
    """
    åˆ†ææ‰€æœ‰æ¸¬è©¦æª”æ¡ˆçš„ç•°å¸¸çª—å£æ¯”ä¾‹

    Args:
        model_dir: æ¨¡å‹ç›®éŒ„è·¯å¾‘
    """
    print("=== è©³ç´°åˆ†ææ‰€æœ‰æ¸¬è©¦æª”æ¡ˆçš„ç•°å¸¸æ¯”ä¾‹ ===\n")
    print(f"ä½¿ç”¨æ¨¡å‹ç›®éŒ„: {model_dir}")

    # æ¨¡å‹åƒæ•¸
    FINAL_MODEL_NAME = "final_model.pth"
    FINAL_SCALER_NAME = "final_scaler.joblib"
    WINDOW_SIZE = 500
    STEP_SIZE = 50

    # å˜—è©¦è¼‰å…¥ç”± K-Fold validation è¨ˆç®—å‡ºçš„ golden threshold
    threshold_info_path = os.path.join(model_dir, "threshold_info.joblib")
    golden_file_threshold = None
    golden_window_threshold = 0.5
    if os.path.exists(threshold_info_path):
        try:
            threshold_info = joblib.load(threshold_info_path)
            golden_file_threshold = float(threshold_info.get("file_level_threshold", 0.5))
            golden_window_threshold = float(threshold_info.get("window_level_threshold", 0.5))
            print(f"è¼‰å…¥ golden threshold æ–¼: {threshold_info_path}")
            print(f"  file_level_threshold = {golden_file_threshold:.4f}")
            print(f"  window_level_threshold = {golden_window_threshold:.4f}")
        except Exception as e:
            print(f"è­¦å‘Š: è®€å– threshold_info.joblib å¤±æ•—: {e}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # è¼‰å…¥ scaler
    scaler_path = os.path.join(model_dir, FINAL_SCALER_NAME)
    if not os.path.exists(scaler_path):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Scaler æ–‡ä»¶: {scaler_path}")
        return

    scaler = data_loader.load_scaler(scaler_path)
    NUM_FEATURES = scaler.n_features_in_  # å¾ scaler ç²å–ç‰¹å¾µæ•¸é‡

    # è¼‰å…¥æ¨¡å‹
    model_instance = model.CNC_1D_CNN(
        num_features=NUM_FEATURES, window_size=WINDOW_SIZE
    ).to(device)

    model_path = os.path.join(model_dir, FINAL_MODEL_NAME)
    if not os.path.exists(model_path):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        return

    model_instance.load_state_dict(torch.load(model_path, map_location=device))
    model_instance.eval()

    print(f"æˆåŠŸè¼‰å…¥æ¨¡å‹å’Œ Scaler")

    # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„æ¸¬è©¦è³‡æ–™
    test_data_path = os.path.join(model_dir, "test_data.joblib")
    if os.path.exists(test_data_path):
        print(f"ä½¿ç”¨æ¨¡å‹ç›®éŒ„ä¸­çš„æ¸¬è©¦è³‡æ–™: {test_data_path}")
        test_data_dict = joblib.load(test_data_path)
        test_data_list = test_data_dict['test_data_list']
        test_labels_list = test_data_dict['test_labels_list']

        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆåç¨±è³‡è¨Š
        if 'test_file_names' in test_data_dict:
            test_file_names = test_data_dict['test_file_names']
        else:
            test_file_names = [f"file_{i+1}" for i in range(len(test_data_list))]
    else:
        print("æœªæ‰¾åˆ°å°æ‡‰çš„æ¸¬è©¦è³‡æ–™ï¼Œä½¿ç”¨é è¨­æ¸¬è©¦ç›®éŒ„")
        test_data_list, test_labels_list = data_loader.load_all_data_from_dir("./data/test")
        test_file_names = [f"file_{i+1}" for i in range(len(test_data_list))]

    if len(test_data_list) == 0:
        print("éŒ¯èª¤: æ‰¾ä¸åˆ°æ¸¬è©¦è³‡æ–™")
        return

    # åˆ†ææ¯å€‹æª”æ¡ˆ
    results = []

    for i, (data_array, true_label) in enumerate(zip(test_data_list, test_labels_list)):
        file_name = test_file_names[i] if i < len(test_file_names) else f"file_{i+1}"

        # è™•ç†å–®ä¸€æª”æ¡ˆ
        scaled_data = scaler.transform(data_array)
        X_windows, _ = data_loader.create_windows([scaled_data], [0], WINDOW_SIZE, STEP_SIZE)

        if len(X_windows) == 0:
            print(f"è­¦å‘Š: æª”æ¡ˆ {file_name} å¤ªçŸ­ï¼Œç„¡æ³•å‰µå»ºçª—å£")
            continue

        X_tensor = torch.tensor(X_windows.transpose(0, 2, 1), dtype=torch.float32).to(device)

        with torch.no_grad():
            outputs = model_instance(X_tensor)
            probs = torch.sigmoid(outputs).cpu().numpy().flatten()

        # è¨ˆç®—ç•°å¸¸æ¯”ä¾‹
        # window å±¤ç´šä½¿ç”¨ threshold_info ä¸­çš„è¨­å®š (è‹¥ç„¡å‰‡ç‚º 0.5)
        abnormal_count = np.sum(probs > golden_window_threshold)
        total_windows = len(probs)
        abnormal_ratio = abnormal_count / total_windows

        # è¨˜éŒ„çµæœ
        file_type = "State1 (Normal)" if true_label == 0 else "State2 (Abnormal)"
        results.append({
            'file_name': file_name,
            'file_index': i + 1,
            'file_type': file_type,
            'true_label': true_label,
            'total_windows': total_windows,
            'abnormal_windows': abnormal_count,
            'abnormal_ratio': abnormal_ratio
        })

        print(f"File {i+1} [{file_name}] - {file_type}:")
        print(f"  Total windows: {total_windows}")
        print(f"  Abnormal windows: {abnormal_count}")
        print(f"  Abnormal ratio: {abnormal_ratio:.3f} ({abnormal_ratio*100:.1f}%)")

        # ä¸åŒé–¾å€¼çš„é æ¸¬çµæœ
        thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
        predictions = []
        for threshold in thresholds:
            pred = 1 if abnormal_ratio > threshold else 0
            correct = "âœ“" if pred == true_label else "âœ—"
            predictions.append(f"{threshold}: {pred}{correct}")
        print(f"  Threshold predictions: {' | '.join(predictions)}")
        print()

    if not results:
        print("æ²’æœ‰å¯åˆ†æçš„æª”æ¡ˆ")
        return

    # ç¸½çµåˆ†æ
    print("=== Threshold Effect Summary ===")

    # å¦‚æœæœ‰ golden thresholdï¼Œå„ªå…ˆé¡¯ç¤ºå…¶æ•ˆæœ
    if golden_file_threshold is not None:
        correct_count = 0
        for result in results:
            pred = 1 if result['abnormal_ratio'] > golden_file_threshold else 0
            if pred == result['true_label']:
                correct_count += 1
        accuracy = correct_count / len(results)
        print(f"ä½¿ç”¨ golden file-level threshold {golden_file_threshold:.3f}: Accuracy {accuracy:.3f} ({correct_count}/{len(results)})")

    # ä»ç„¶å¯ä»¥æƒä¸€çµ„å›ºå®š threshold ä½œç‚ºåƒè€ƒ
    thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
    best_threshold = thresholds[0]
    best_accuracy = 0.0

    for threshold in thresholds:
        correct_count = 0
        for result in results:
            pred = 1 if result['abnormal_ratio'] > threshold else 0
            if pred == result['true_label']:
                correct_count += 1
        accuracy = correct_count / len(results)
        print(f"Threshold {threshold}: Accuracy {accuracy:.3f} ({correct_count}/{len(results)})")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_threshold = threshold

    print(f"\nBest threshold (grid [0.1,0.3,0.5,0.7,0.9]): {best_threshold} with accuracy {best_accuracy:.3f}")

    # è©³ç´°åˆ†æ
    print(f"\n=== Detailed Analysis ===")
    state1_ratios = [r['abnormal_ratio'] for r in results if r['true_label'] == 0]
    state2_ratios = [r['abnormal_ratio'] for r in results if r['true_label'] == 1]

    if state1_ratios:
        print(f"Normal files (State1) abnormal ratios:")
        for i, ratio in enumerate(state1_ratios):
            print(f"  File {i+1}: {ratio:.3f} ({ratio*100:.1f}%)")
        print(f"  Max: {max(state1_ratios):.3f}")

    if state2_ratios:
        print(f"\nAbnormal files (State2) abnormal ratios:")
        for i, ratio in enumerate(state2_ratios):
            print(f"  File {len(state1_ratios)+i+1}: {ratio:.3f} ({ratio*100:.1f}%)")
        print(f"  Min: {min(state2_ratios):.3f}")

    if state1_ratios and state2_ratios:
        print(f"\nğŸ’¡ Key insights:")
        print(f"- Normal files max abnormal ratio: {max(state1_ratios):.3f}")
        print(f"- Abnormal files min abnormal ratio: {min(state2_ratios):.3f}")
        print(f"- Optimal threshold should be between {max(state1_ratios):.3f} and {min(state2_ratios):.3f}")

    return results

def main():
    parser = argparse.ArgumentParser(description="Analyze threshold effects on test files")
    parser.add_argument(
        "--model-dir",
        type=str,
        default="./models",
        help="Path to model directory (default: ./models)"
    )

    args = parser.parse_args()

    analyze_all_test_files(args.model_dir)

if __name__ == "__main__":
    main()
