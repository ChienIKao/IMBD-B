#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æª”æ¡ˆç•°å¸¸æ¯”ä¾‹åˆ†æå·¥å…· - å¯ç”¨æ–¼ä»»ä½• CSV æª”æ¡ˆ (ä¸éœ€è¦çŸ¥é“ç­”æ¡ˆ)

ç”¨é€”:
1. åˆ†æå–®ä¸€æª”æ¡ˆæˆ–æ•´å€‹è³‡æ–™å¤¾çš„ç•°å¸¸çª—å£æ¯”ä¾‹
2. æŸ¥çœ‹æ¯å€‹æª”æ¡ˆåœ¨ä¸åŒ threshold ä¸‹æœƒè¢«é æ¸¬ç‚ºä»€éº¼
3. æ¯”è³½æ™‚ç”¨ä¾†æª¢æŸ¥é æ¸¬çš„åˆç†æ€§

ä½¿ç”¨æ–¹å¼:
  # åˆ†æå–®ä¸€æª”æ¡ˆ
  python analyze_files.py --file æª”æ¡ˆ.csv --model-dir ./models/training_XXX

  # åˆ†ææ•´å€‹è³‡æ–™å¤¾
  python analyze_files.py --dir è³‡æ–™å¤¾/ --model-dir ./models/training_XXX

  # æŒ‡å®šè¦æ¸¬è©¦çš„ threshold
  python analyze_files.py --dir è³‡æ–™å¤¾/ --thresholds 0.05 0.1 0.15 0.2
"""

import os
import torch
import numpy as np
import argparse
import pandas as pd
from src import data_loader, model

def analyze_file(file_path, model_instance, scaler, device, window_size=500, step_size=50):
    """
    åˆ†æå–®ä¸€æª”æ¡ˆçš„ç•°å¸¸çª—å£æ¯”ä¾‹

    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        model_instance: è¨“ç·´å¥½çš„æ¨¡å‹
        scaler: è¨“ç·´å¥½çš„ scaler
        device: è¨ˆç®—è£ç½®
        window_size: çª—å£å¤§å°
        step_size: æ­¥é•·

    Returns:
        dict: åŒ…å«åˆ†æçµæœçš„å­—å…¸
    """
    try:
        # è¼‰å…¥ä¸¦è™•ç†è³‡æ–™
        data_array = data_loader.load_single_csv(file_path)
        scaled_data = scaler.transform(data_array)
        X_windows, _ = data_loader.create_windows([scaled_data], [0], window_size, step_size)

        if len(X_windows) == 0:
            return {
                'file_name': os.path.basename(file_path),
                'error': 'File too short to create windows'
            }

        # é æ¸¬
        X_tensor = torch.tensor(X_windows.transpose(0, 2, 1), dtype=torch.float32).to(device)

        with torch.no_grad():
            outputs = model_instance(X_tensor)
            probs = torch.sigmoid(outputs).cpu().numpy().flatten()

        # è¨ˆç®—çµ±è¨ˆ
        abnormal_count = np.sum(probs > 0.5)
        total_windows = len(probs)
        abnormal_ratio = abnormal_count / total_windows

        # è¨ˆç®—ç•°å¸¸æ©Ÿç‡çš„åˆ†å¸ƒ
        prob_stats = {
            'mean': float(np.mean(probs)),
            'std': float(np.std(probs)),
            'min': float(np.min(probs)),
            'max': float(np.max(probs)),
            'median': float(np.median(probs))
        }

        return {
            'file_name': os.path.basename(file_path),
            'file_path': file_path,
            'total_windows': total_windows,
            'abnormal_windows': abnormal_count,
            'abnormal_ratio': abnormal_ratio,
            'prob_stats': prob_stats,
            'all_probs': probs
        }

    except Exception as e:
        return {
            'file_name': os.path.basename(file_path),
            'error': str(e)
        }

def analyze_files(file_paths, model_dir, thresholds=[0.05, 0.1, 0.15, 0.2, 0.3]):
    """
    åˆ†æå¤šå€‹æª”æ¡ˆçš„ç•°å¸¸æ¯”ä¾‹

    Args:
        file_paths: CSV æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        model_dir: æ¨¡å‹ç›®éŒ„
        thresholds: è¦æ¸¬è©¦çš„ threshold åˆ—è¡¨
    """
    print("=" * 70)
    print("æª”æ¡ˆç•°å¸¸æ¯”ä¾‹åˆ†æå·¥å…·")
    print("=" * 70)
    print(f"\næ¨¡å‹ç›®éŒ„: {model_dir}")
    print(f"è¦åˆ†æçš„æª”æ¡ˆæ•¸é‡: {len(file_paths)}")
    print(f"æ¸¬è©¦çš„ thresholds: {thresholds}\n")

    # è¼‰å…¥æ¨¡å‹å’Œ scaler
    FINAL_MODEL_NAME = "final_model.pth"
    FINAL_SCALER_NAME = "final_scaler.joblib"
    WINDOW_SIZE = 500
    STEP_SIZE = 50

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # è¼‰å…¥ scaler
    scaler_path = os.path.join(model_dir, FINAL_SCALER_NAME)
    if not os.path.exists(scaler_path):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° Scaler: {scaler_path}")
        return

    scaler = data_loader.load_scaler(scaler_path)
    NUM_FEATURES = scaler.n_features_in_

    # è¼‰å…¥æ¨¡å‹
    model_instance = model.CNC_1D_CNN(
        num_features=NUM_FEATURES, window_size=WINDOW_SIZE
    ).to(device)

    model_path = os.path.join(model_dir, FINAL_MODEL_NAME)
    if not os.path.exists(model_path):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}")
        return

    model_instance.load_state_dict(torch.load(model_path, map_location=device))
    model_instance.eval()

    print("âœ“ æˆåŠŸè¼‰å…¥æ¨¡å‹å’Œ Scaler\n")
    print("=" * 70)
    print("é–‹å§‹åˆ†æ...")
    print("=" * 70)

    # åˆ†ææ‰€æœ‰æª”æ¡ˆ
    results = []

    for i, file_path in enumerate(file_paths, 1):
        print(f"\n[{i}/{len(file_paths)}] {os.path.basename(file_path)}")

        result = analyze_file(file_path, model_instance, scaler, device, WINDOW_SIZE, STEP_SIZE)

        if 'error' in result:
            print(f"  âŒ éŒ¯èª¤: {result['error']}")
            continue

        results.append(result)

        # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
        print(f"  ç¸½çª—å£æ•¸: {result['total_windows']}")
        print(f"  ç•°å¸¸çª—å£æ•¸: {result['abnormal_windows']}")
        print(f"  ç•°å¸¸æ¯”ä¾‹: {result['abnormal_ratio']:.3f} ({result['abnormal_ratio']*100:.1f}%)")

        # é¡¯ç¤ºæ©Ÿç‡çµ±è¨ˆ
        prob_stats = result['prob_stats']
        print(f"  æ©Ÿç‡çµ±è¨ˆ: mean={prob_stats['mean']:.3f}, std={prob_stats['std']:.3f}, "
              f"min={prob_stats['min']:.3f}, max={prob_stats['max']:.3f}")

        # é¡¯ç¤ºä¸åŒ threshold çš„é æ¸¬çµæœ
        predictions = []
        for th in thresholds:
            pred = 1 if result['abnormal_ratio'] > th else 0
            pred_label = 'state2' if pred == 1 else 'state1'
            predictions.append(f"{th}: {pred_label}")
        print(f"  é æ¸¬çµæœ: {' | '.join(predictions)}")

    if not results:
        print("\næ²’æœ‰æˆåŠŸåˆ†æçš„æª”æ¡ˆ")
        return

    # ç¸½çµåˆ†æ
    print("\n" + "=" * 70)
    print("ç¸½çµåˆ†æ")
    print("=" * 70)

    # çµ±è¨ˆä¸åŒ threshold ä¸‹çš„é æ¸¬åˆ†å¸ƒ
    for th in thresholds:
        state1_count = sum(1 for r in results if r['abnormal_ratio'] <= th)
        state2_count = len(results) - state1_count
        print(f"\nThreshold = {th}:")
        print(f"  é æ¸¬ç‚º state1 (æ­£å¸¸): {state1_count} å€‹æª”æ¡ˆ")
        print(f"  é æ¸¬ç‚º state2 (ç•°å¸¸): {state2_count} å€‹æª”æ¡ˆ")
        print(f"  ç•°å¸¸æ¯”ä¾‹: {state2_count/len(results)*100:.1f}%")

    # ç•°å¸¸æ¯”ä¾‹åˆ†å¸ƒ
    ratios = [r['abnormal_ratio'] for r in results]
    print(f"\nç•°å¸¸æ¯”ä¾‹çµ±è¨ˆ:")
    print(f"  æœ€å°å€¼: {min(ratios):.3f}")
    print(f"  æœ€å¤§å€¼: {max(ratios):.3f}")
    print(f"  å¹³å‡å€¼: {np.mean(ratios):.3f}")
    print(f"  ä¸­ä½æ•¸: {np.median(ratios):.3f}")
    print(f"  æ¨™æº–å·®: {np.std(ratios):.3f}")

    # å„²å­˜è©³ç´°çµæœåˆ° CSV
    df_results = pd.DataFrame([
        {
            'file_name': r['file_name'],
            'total_windows': r['total_windows'],
            'abnormal_windows': r['abnormal_windows'],
            'abnormal_ratio': r['abnormal_ratio'],
            'prob_mean': r['prob_stats']['mean'],
            'prob_std': r['prob_stats']['std'],
            'prob_min': r['prob_stats']['min'],
            'prob_max': r['prob_stats']['max'],
            **{f'pred_th_{th}': 1 if r['abnormal_ratio'] > th else 0 for th in thresholds}
        }
        for r in results
    ])

    output_file = 'analysis_results.csv'
    df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ è©³ç´°çµæœå·²å„²å­˜è‡³: {output_file}")

    # é¡¯ç¤ºå»ºè­°
    print("\n" + "=" * 70)
    print("ğŸ’¡ å»ºè­°")
    print("=" * 70)

    # æ‰¾å‡ºå¯èƒ½æœ‰å•é¡Œçš„æª”æ¡ˆ
    mean_ratio = np.mean(ratios)
    std_ratio = np.std(ratios)

    outliers = [r for r in results if abs(r['abnormal_ratio'] - mean_ratio) > 2 * std_ratio]
    if outliers:
        print(f"\nâš ï¸  ç™¼ç¾ {len(outliers)} å€‹ç•°å¸¸å€¼æª”æ¡ˆ (ç•°å¸¸æ¯”ä¾‹èˆ‡å¹³å‡å€¼ç›¸å·® > 2 å€‹æ¨™æº–å·®):")
        for r in outliers:
            print(f"  - {r['file_name']}: {r['abnormal_ratio']:.3f}")

    # Threshold å»ºè­°
    print(f"\næ ¹æ“šåˆ†æçµæœ:")
    print(f"  - å¦‚æœæ‚¨çš„è³‡æ–™æ‡‰è©²å¤§éƒ¨åˆ†æ˜¯æ­£å¸¸çš„,è€ƒæ…®ä½¿ç”¨è¼ƒä½çš„ threshold (0.1-0.15)")
    print(f"  - å¦‚æœæ‚¨çš„è³‡æ–™æ‡‰è©²æœ‰è¼ƒå¤šç•°å¸¸,è€ƒæ…®ä½¿ç”¨è¼ƒé«˜çš„ threshold (0.2-0.3)")
    print(f"  - ç•¶å‰è³‡æ–™çš„å¹³å‡ç•°å¸¸æ¯”ä¾‹ç‚º {np.mean(ratios):.3f}")

def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ CSV æª”æ¡ˆçš„ç•°å¸¸çª—å£æ¯”ä¾‹ (ä¸éœ€è¦çŸ¥é“ç­”æ¡ˆ)"
    )

    # è¼¸å…¥é¸é …: å–®ä¸€æª”æ¡ˆæˆ–è³‡æ–™å¤¾
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--file",
        type=str,
        help="è¦åˆ†æçš„å–®ä¸€ CSV æª”æ¡ˆ"
    )
    group.add_argument(
        "--dir",
        type=str,
        help="è¦åˆ†æçš„è³‡æ–™å¤¾ (åŒ…å«å¤šå€‹ CSV æª”æ¡ˆ)"
    )

    parser.add_argument(
        "--model-dir",
        type=str,
        default="./models",
        help="æ¨¡å‹ç›®éŒ„è·¯å¾‘ (default: ./models)"
    )

    parser.add_argument(
        "--thresholds",
        nargs='+',
        type=float,
        default=[0.05, 0.1, 0.15, 0.2, 0.3],
        help="è¦æ¸¬è©¦çš„ threshold åˆ—è¡¨ (default: 0.05 0.1 0.15 0.2 0.3)"
    )

    args = parser.parse_args()

    # æ”¶é›†è¦åˆ†æçš„æª”æ¡ˆ
    file_paths = []

    if args.file:
        if not os.path.exists(args.file):
            print(f"âŒ éŒ¯èª¤: æª”æ¡ˆä¸å­˜åœ¨: {args.file}")
            return
        file_paths = [args.file]

    elif args.dir:
        if not os.path.exists(args.dir):
            print(f"âŒ éŒ¯èª¤: è³‡æ–™å¤¾ä¸å­˜åœ¨: {args.dir}")
            return

        # æ‰¾å‡ºæ‰€æœ‰ CSV æª”æ¡ˆ
        for root, dirs, files in os.walk(args.dir):
            for file in files:
                if file.endswith('.csv'):
                    file_paths.append(os.path.join(root, file))

        if not file_paths:
            print(f"âŒ éŒ¯èª¤: åœ¨ {args.dir} ä¸­æ‰¾ä¸åˆ°ä»»ä½• CSV æª”æ¡ˆ")
            return

    # åŸ·è¡Œåˆ†æ
    analyze_files(file_paths, args.model_dir, args.thresholds)

if __name__ == "__main__":
    main()
