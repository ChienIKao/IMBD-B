# Grad-CAM Technical Implementation Details

## ğŸ”¬ **å…§éƒ¨æŠ€è¡“ç´°ç¯€èˆ‡æ¼”ç®—æ³•å¯¦ç¾**

### ğŸ“‹ **ç›®éŒ„**
1. [æ ¸å¿ƒæ¶æ§‹è¨­è¨ˆ](#æ ¸å¿ƒæ¶æ§‹è¨­è¨ˆ)
2. [æ¼”ç®—æ³•å¯¦ç¾ç´°ç¯€](#æ¼”ç®—æ³•å¯¦ç¾ç´°ç¯€)
3. [æ•¸æ“šè™•ç†ç®¡ç·š](#æ•¸æ“šè™•ç†ç®¡ç·š)
4. [è¦–è¦ºåŒ–æ¸²æŸ“å¼•æ“](#è¦–è¦ºåŒ–æ¸²æŸ“å¼•æ“)
5. [æ•ˆèƒ½å„ªåŒ–ç­–ç•¥](#æ•ˆèƒ½å„ªåŒ–ç­–ç•¥)
6. [ç¨‹å¼ç¢¼æ¶æ§‹åˆ†æ](#ç¨‹å¼ç¢¼æ¶æ§‹åˆ†æ)

---

## ğŸ—ï¸ **æ ¸å¿ƒæ¶æ§‹è¨­è¨ˆ**

### **æ•´é«”æ¶æ§‹åœ–**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Input    â”‚â”€â”€â”€â–¶â”‚  Preprocessing   â”‚â”€â”€â”€â–¶â”‚ Model Loading   â”‚
â”‚  - CSV Reader   â”‚    â”‚  - Standardizer  â”‚    â”‚ - 1D-CNN Model  â”‚
â”‚  - Validation   â”‚    â”‚  - Window Split  â”‚    â”‚ - Scaler Load   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grad-CAM Engine â”‚    â”‚ Attention Calc   â”‚    â”‚ Visualization   â”‚
â”‚ - Hook Register â”‚    â”‚ - Regional Enh   â”‚    â”‚ - Plot Engine   â”‚
â”‚ - Gradient Flow â”‚    â”‚ - Downsampling   â”‚    â”‚ - Export System â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **æ¨¡çµ„ç›¸ä¾é—œä¿‚**
```python
grad_cam_visualization.py
â”œâ”€â”€ torch (PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶)
â”œâ”€â”€ numpy (æ•¸å€¼è¨ˆç®—)
â”œâ”€â”€ matplotlib (è¦–è¦ºåŒ–)
â”œâ”€â”€ src.model (è‡ªå®šç¾© 1D-CNN æ¨¡å‹)
â”œâ”€â”€ src.data_loader (è³‡æ–™è¼‰å…¥å™¨)
â””â”€â”€ joblib (æ¨¡å‹åºåˆ—åŒ–)
```

---

## âš™ï¸ **æ¼”ç®—æ³•å¯¦ç¾ç´°ç¯€**

### **1. Grad-CAM æ ¸å¿ƒæ¼”ç®—æ³•**

#### **Hook æ©Ÿåˆ¶å¯¦ç¾**
```python
def compute_grad_cam_for_window(model_instance, window, target_layer_name="conv3"):
    # 1. å»ºç«‹æ¢¯åº¦å’Œæ¿€æ´»å€¼å®¹å™¨
    activations = None
    gradients = None
    
    # 2. å®šç¾©å‰å‘ Hook (æ•ç²æ¿€æ´»å€¼)
    def forward_hook(module, input, output):
        nonlocal activations
        activations = output  # Shape: (1, channels, length)
    
    # 3. å®šç¾©åå‘ Hook (æ•ç²æ¢¯åº¦)
    def backward_hook(module, grad_input, grad_output):
        nonlocal gradients
        gradients = grad_output[0]  # Shape: (1, channels, length)
    
    # 4. è¨»å†Š Hook åˆ°ç›®æ¨™å±¤
    target_layer = getattr(model_instance, target_layer_name)
    forward_handle = target_layer.register_forward_hook(forward_hook)
    backward_handle = target_layer.register_backward_hook(backward_hook)
```

#### **CAM è¨ˆç®—æ•¸å­¸å…¬å¼**
```python
# 1. å…¨åŸŸå¹³å‡æ± åŒ–è¨ˆç®—æ¬Šé‡
Î±_c = (1/Z) * Î£(âˆ‚y_c/âˆ‚A_ij)  # Z = width Ã— height

# 2. åŠ æ¬Šçµ„åˆç”Ÿæˆ CAM
L_CAM = Î£(Î±_c Ã— A_c)

# 3. ReLU æ¿€æ´»ä¿ç•™æ­£å€¼
L_CAM = ReLU(L_CAM)

# 4. Min-Max æ­¸ä¸€åŒ–åˆ° [0,1]
L_CAM_norm = (L_CAM - min) / (max - min)
```

#### **å¯¦éš›ç¨‹å¼ç¢¼å¯¦ç¾**
```python
# è¨ˆç®—å…¨åŸŸå¹³å‡æ± åŒ–æ¬Šé‡
alpha = torch.mean(gradients, dim=2, keepdim=True)  # (1, channels, 1)

# åŠ æ¬Šçµ„åˆ
cam = torch.sum(alpha * activations, dim=1)  # (1, length)

# ReLU æ¿€æ´»
cam = F.relu(cam)  # åªä¿ç•™æ­£å€¼

# æ’å€¼åˆ°æ¨™æº–é•·åº¦
if cam.size(0) != WINDOW_SIZE:
    cam = F.interpolate(cam.unsqueeze(0).unsqueeze(0), 
                       size=WINDOW_SIZE, mode='linear')

# æ­¸ä¸€åŒ–
cam_np = cam.detach().cpu().numpy()
cam_normalized = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min())
```

### **2. æ»‘å‹•çª—å£èšåˆæ¼”ç®—æ³•**

#### **é‡ç–Šçª—å£è™•ç†**
```python
def aggregate_global_cam(windows, start_indices, model_instance, total_length):
    global_cam = np.zeros(total_length)
    count = np.zeros(total_length)
    
    for k, (window, start) in enumerate(zip(windows, start_indices)):
        # è¨ˆç®—å–®å€‹çª—å£çš„ CAM
        cam_k = compute_grad_cam_for_window(model_instance, window)
        
        # ç´¯ç©åˆ°å…¨åŸŸ CAM
        for i in range(WINDOW_SIZE):
            global_idx = start + i
            if global_idx < total_length:
                global_cam[global_idx] += cam_k[i]
                count[global_idx] += 1
    
    # å¹³å‡åŒ–è™•ç†
    mask = count > 0
    global_cam[mask] /= count[mask]
    
    return global_cam
```

#### **è¦†è“‹åº¦åˆ†æ**
```
çª—å£è¨­å®š:
- çª—å£å¤§å°: 500 æ™‚é–“æ­¥
- æ»‘å‹•æ­¥é•·: 50 æ™‚é–“æ­¥  
- é‡ç–Šæ¯”ä¾‹: 90% (450/500)

è¦†è“‹æ¨¡å¼:
- èµ·å§‹å€åŸŸ (0-499): è¦†è“‹åº¦ 1-10
- ä¸­å¿ƒå€åŸŸ (500-T-500): è¦†è“‹åº¦æ†å®š 10
- çµæŸå€åŸŸ (T-499-T): è¦†è“‹åº¦ 10-1
```

### **3. å€åŸŸå¢å¼·æ¼”ç®—æ³•**

#### **æ»‘å‹•å€åŸŸå¢å¼·**
```python
def enhance_attention_regionally(attention_values, window_size=500):
    T = len(attention_values)
    enhanced_attention = np.zeros_like(attention_values)
    
    for center_idx in range(T):
        # å®šç¾©é„°åŸŸçª—å£
        half_window = window_size // 2
        start_idx = max(0, center_idx - half_window)
        end_idx = min(T, center_idx + half_window)
        
        region = attention_values[start_idx:end_idx]
        max_attention = np.max(region)
        
        if max_attention > 0.05:  # é–¾å€¼éæ¿¾
            # æ‰¾åˆ°å€åŸŸå³°å€¼ä½ç½®
            max_idx = np.argmax(region)
            center_relative = center_idx - start_idx
            
            # è¨ˆç®—è·é›¢å³°å€¼çš„è·é›¢
            distance_to_peak = abs(center_relative - max_idx)
            
            # é«˜æ–¯å¢å¼·å› å­
            enhancement_factor = np.exp(-distance_to_peak**2 / (2 * (window_size/8)**2))
            
            # å¢å¼·å…¬å¼
            enhancement = 0.3 * enhancement_factor * max_attention
            enhanced_value = attention_values[center_idx] * (1 + enhancement)
            
            enhanced_attention[center_idx] = np.clip(enhanced_value, 0, 1)
        else:
            enhanced_attention[center_idx] = attention_values[center_idx]
    
    return enhanced_attention
```

### **4. å±€éƒ¨æœ€å¤§å€¼å–æ¨£æ¼”ç®—æ³•**

#### **å–æ¨£ç­–ç•¥**
```python
def downsample_and_interpolate(values, sample_step=200, local_window=200):
    T = len(values)
    half_window = local_window // 2
    
    # å»ºç«‹å–æ¨£é»ç´¢å¼•
    sample_indices = np.arange(0, T, sample_step)
    if sample_indices[-1] != T - 1:
        sample_indices = np.append(sample_indices, T - 1)
    
    sampled_values = []
    
    # å°æ¯å€‹å–æ¨£é»æå–å±€éƒ¨æœ€å¤§å€¼
    for idx in sample_indices:
        start_idx = max(0, idx - half_window)
        end_idx = min(T, idx + half_window + 1)
        
        local_window_values = values[start_idx:end_idx]
        max_value = np.max(local_window_values)
        
        sampled_values.append(max_value)
    
    # ç·šæ€§æ’å€¼å›åˆ°åŸå§‹é•·åº¦
    interpolated = np.interp(np.arange(T), sample_indices, sampled_values)
    
    return interpolated
```

---

## ğŸ”„ **æ•¸æ“šè™•ç†ç®¡ç·š**

### **1. CSV è®€å–èˆ‡é è™•ç†**

#### **è³‡æ–™è¼‰å…¥æµç¨‹**
```python
def read_and_preprocess_csv(csv_path, scaler):
    # 1. è®€å– CSV (è·³éå–®ä½è¡Œ)
    df = pd.read_csv(csv_path, header=0, skiprows=[1])
    
    # 2. æå–ç‰¹å¾µåç¨± (å¾ç¬¬ C æ¬„é–‹å§‹)
    feature_names = df.columns[2:2+NUM_FEATURES].tolist()
    
    # 3. ä½¿ç”¨å°ˆæ¡ˆè³‡æ–™è¼‰å…¥å™¨
    data_array = data_loader.load_single_csv(csv_path)
    
    # 4. æ‡‰ç”¨è¨“ç·´æ™‚çš„æ¨™æº–åŒ–å™¨ (ä¸èƒ½é‡æ–°è¨“ç·´)
    scaled_data = scaler.transform(data_array)
    
    return scaled_data, feature_names
```

#### **è³‡æ–™æ ¼å¼è¦æ±‚**
```
CSV çµæ§‹:
Row 1: æ¬„ä½åç¨± (A, B, C, D, E, ...)
Row 2: å–®ä½è³‡è¨Š (è·³é)
Row 3+: å¯¦éš›æ•¸æ“š

ç‰¹å¾µæå–:
- èµ·å§‹æ¬„ä½: C (ç´¢å¼• 2)
- ç‰¹å¾µæ•¸é‡: 35 å€‹
- ç›®æ¨™ç‰¹å¾µ: POSFN, POS3DC.1, TCMD, SVPOS ç­‰
```

### **2. æ»‘å‹•çª—å£æå–**

#### **çª—å£ç”Ÿæˆé‚è¼¯**
```python
def extract_windows(data_array, window_length=500, stride=50):
    T, num_features = data_array.shape
    windows = []
    start_indices = []
    
    for start in range(0, T - window_length + 1, stride):
        end = start + window_length
        window = data_array[start:end, :]  # (500, 35)
        
        # è½‰æ›ç‚º PyTorch Conv1D æ ¼å¼ (35, 500)
        window_conv1d = window.transpose(1, 0)
        
        windows.append(window_conv1d)
        start_indices.append(start)
    
    return np.array(windows), start_indices
```

### **3. ç‰¹å¾µé‡è¦æ€§è¨ˆç®—**

#### **é€šé“æ³¨æ„åŠ›è¨ˆç®—**
```python
def compute_channel_attention(data_array, global_cam):
    # è¨ˆç®—é‡è¦æ€§çŸ©é™£
    importance = np.abs(data_array) * global_cam[:, np.newaxis]  # (T, 35)
    
    # æ¯å€‹é€šé“çš„ Min-Max æ­¸ä¸€åŒ–
    channel_attention = np.zeros((num_features, T))
    
    for c in range(num_features):
        importance_c = importance[:, c]
        imp_min, imp_max = importance_c.min(), importance_c.max()
        
        if imp_max > imp_min:
            channel_attention[c, :] = (importance_c - imp_min) / (imp_max - imp_min)
        else:
            channel_attention[c, :] = np.zeros_like(importance_c)
    
    return channel_attention
```

---

## ğŸ¨ **è¦–è¦ºåŒ–æ¸²æŸ“å¼•æ“**

### **1. é›™è»¸åœ–è¡¨è¨­è¨ˆ**

#### **ä¸»è»¸èˆ‡å‰¯è»¸è¨­å®š**
```python
def plot_line_with_attention(data_array, channel_attention, channel_idx, ...):
    # å»ºç«‹åœ–è¡¨
    fig, ax1 = plt.subplots(1, 1, figsize=(16, 8))
    
    # ä¸»è»¸: ä¿¡è™Ÿæ•¸æ“š (è—è‰²)
    ax1.plot(time_axis, data_array[:, channel_idx], 
            color='blue', linewidth=0.8, alpha=0.9,
            label=f'Signal ({feature_name})')
    
    # å»ºç«‹å‰¯è»¸: æ³¨æ„åŠ›åˆ†æ•¸ (ç´…è‰²)
    ax1_twin = ax1.twinx()
    ax1_twin.plot(time_axis, attention_values, 
                  color='red', linewidth=2.0, alpha=0.5,
                  label='Attention Score')
```

### **2. ç†±åŠ›åœ–èƒŒæ™¯æ¸²æŸ“**

#### **é€£çºŒç†±åŠ›åœ–å¯¦ç¾**
```python
# å–å¾— Y è»¸ç¯„åœ
y_min, y_max = ax1.get_ylim()

# é‡å¡‘æ³¨æ„åŠ›æ•¸æ“šç‚º imshow æ ¼å¼
attention_heatmap = attention_values.reshape(1, -1)  # (1, T)

# ç¹ªè£½èƒŒæ™¯ç†±åŠ›åœ–
im_bg = ax1.imshow(attention_heatmap, 
                  cmap='Reds',              # ç´…è‰²è‰²è­œ
                  aspect='auto',            # è‡ªå‹•ç¸±æ©«æ¯”
                  interpolation='bilinear', # é›™ç·šæ€§æ’å€¼
                  alpha=0.6,               # 60% ä¸é€æ˜åº¦
                  extent=[0, T, y_min, y_max],  # ç¯„åœå°é½Š
                  zorder=0)                # èƒŒæ™¯å±¤
```

### **3. åœ–è¡¨ç¾åŒ–èˆ‡é…ç½®**

#### **å°ˆæ¥­åŒ–è¨­å®š**
```python
# è»¸æ¨™ç±¤èˆ‡é¡è‰²
ax1.set_ylabel('Signal Value', fontsize=12, color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1_twin.set_ylabel('Attention Score', fontsize=12, color='red')
ax1_twin.tick_params(axis='y', labelcolor='red')

# ç¶²æ ¼èˆ‡ç¯„åœ
ax1.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)
ax1_twin.set_ylim(0, 1.0)

# çµ„åˆåœ–ä¾‹
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax1_twin.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

# é«˜å“è³ªè¼¸å‡º
plt.savefig(save_path, dpi=300, bbox_inches='tight', 
            facecolor='white', edgecolor='none')
```

---

## ğŸš€ **æ•ˆèƒ½å„ªåŒ–ç­–ç•¥**

### **1. è¨˜æ†¶é«”ç®¡ç†**

#### **é€çª—å£è™•ç†**
```python
# é¿å…æ‰¹æ¬¡è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”
for k, (window, start) in enumerate(zip(windows, start_indices)):
    # å–®çª—å£è™•ç†
    window_batch = window[np.newaxis, :]  # (1, 35, 500)
    cam_k = compute_grad_cam_for_window(model_instance, window_batch)
    
    # ç«‹å³ç´¯ç©ï¼Œä¸ä¿å­˜ä¸­é–“çµæœ
    for i in range(WINDOW_SIZE):
        global_idx = start + i
        if global_idx < total_length:
            global_cam[global_idx] += cam_k[i]
            count[global_idx] += 1
```

#### **Hook æ¸…ç†æ©Ÿåˆ¶**
```python
try:
    # Grad-CAM è¨ˆç®—
    forward_handle = target_layer.register_forward_hook(forward_hook)
    backward_handle = target_layer.register_backward_hook(backward_hook)
    
    # ... è¨ˆç®—éç¨‹ ...
    
finally:
    # ç¢ºä¿ Hook è¢«æ¸…ç†
    forward_handle.remove()
    backward_handle.remove()
```

### **2. è¨ˆç®—æ•ˆç‡å„ªåŒ–**

#### **GPU è‡ªå‹•åµæ¸¬**
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_instance = model_instance.to(device)
window_tensor = window_tensor.to(device)
```

#### **æ•¸æ“šå£“ç¸®ç­–ç•¥**
```python
# 27,722 â†’ 140 æ¡æ¨£é» (99.5% å£“ç¸®)
sample_step = 200
compression_ratio = len(original_data) / len(sampled_data)
# å¤§å¹…æ¸›å°‘æ¸²æŸ“è¤‡é›œåº¦
```

### **3. è¦–è¦ºåŒ–æ•ˆèƒ½**

#### **æŸµæ ¼åŒ–è™•ç†**
```python
ax1.plot(..., rasterized=True)  # å‘é‡è½‰æŸµæ ¼
im_bg = ax1.imshow(..., rasterized=True)  # ç†±åŠ›åœ–æŸµæ ¼åŒ–
```

#### **è¼¸å‡ºå„ªåŒ–**
```python
plt.savefig(save_path, 
           dpi=300,              # é«˜è§£æåº¦
           bbox_inches='tight',   # ç·Šæ¹Šé‚Šç•Œ
           facecolor='white',     # ç™½è‰²èƒŒæ™¯
           edgecolor='none')      # ç„¡é‚Šæ¡†
```

---

## ğŸ“ **ç¨‹å¼ç¢¼æ¶æ§‹åˆ†æ**

### **1. å‡½æ•¸è·è²¬åˆ†å·¥**

```python
# æ¨¡å‹è¼‰å…¥å±¤
load_model()           # è¼‰å…¥ PyTorch æ¨¡å‹
load_scaler()          # è¼‰å…¥æ¨™æº–åŒ–å™¨

# è³‡æ–™è™•ç†å±¤  
read_and_preprocess_csv()    # CSV è®€å–èˆ‡é è™•ç†
extract_windows()            # æ»‘å‹•çª—å£æå–

# æ ¸å¿ƒè¨ˆç®—å±¤
compute_grad_cam_for_window()     # å–®çª—å£ Grad-CAM
aggregate_global_cam()            # å¤šçª—å£èšåˆ
enhance_attention_regionally()    # å€åŸŸå¢å¼·
downsample_and_interpolate()      # é™å–æ¨£èˆ‡æ’å€¼

# åˆ†æè¨ˆç®—å±¤
compute_channel_attention()       # é€šé“æ³¨æ„åŠ›
compute_feature_importance()      # ç‰¹å¾µé‡è¦æ€§

# è¦–è¦ºåŒ–å±¤
plot_line_with_attention()        # ä¸»è¦–è¦ºåŒ–å‡½æ•¸
print_top_features()              # é‡è¦æ€§æ’åè¼¸å‡º

# ä¸»æ§åˆ¶å±¤
main()                           # ä¸»ç¨‹å¼æµç¨‹æ§åˆ¶
```

### **2. è³‡æ–™æµå‘åˆ†æ**

```
CSV Input â†’ StandardScaler â†’ Sliding Windows
    â†“
Single Window Grad-CAM â†’ Global Aggregation
    â†“  
Regional Enhancement â†’ Downsampling & Interpolation
    â†“
Channel Attention â†’ Feature Importance Ranking
    â†“
Visualization Rendering â†’ File Output (PNG + NPZ)
```

### **3. éŒ¯èª¤è™•ç†æ©Ÿåˆ¶**

```python
try:
    # ä¸»è¦è¨ˆç®—æµç¨‹
    model_instance = load_model(args.model_dir)
    # ...
except FileNotFoundError as e:
    print(f"âŒ File not found: {e}")
    sys.exit(1)
except Exception as e:
    print(f"âŒ Error occurred: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
```

### **4. åƒæ•¸é©—è­‰ç³»çµ±**

```python
# ç‰¹å¾µåç¨±é©—è­‰
if args.feature_name is not None:
    try:
        target_feature = feature_names.index(args.feature_name)
    except ValueError:
        print(f"âš ï¸ Feature name '{args.feature_name}' not found")
        # è‡ªå‹•å›é€€åˆ°æœ€é‡è¦ç‰¹å¾µ
        target_feature = np.argmax(importance_dict['max_scores'])

# æª”æ¡ˆå­˜åœ¨æ€§æª¢æŸ¥
if not os.path.exists(scaler_path):
    raise FileNotFoundError(f"Scaler not found: {scaler_path}")
```

---

## ğŸ”§ **å¯è‡ªè¨‚åƒæ•¸**

### **æ ¸å¿ƒæ¼”ç®—æ³•åƒæ•¸**
```python
# Grad-CAM åƒæ•¸
WINDOW_SIZE = 500      # çª—å£å¤§å°
STEP_SIZE = 50         # æ»‘å‹•æ­¥é•·
NUM_FEATURES = 35      # ç‰¹å¾µæ•¸é‡

# å¢å¼·åƒæ•¸
window_size = 500      # å€åŸŸå¢å¼·çª—å£
enhancement = 0.3      # å¢å¼·å¼·åº¦
threshold = 0.05       # å¢å¼·é–¾å€¼

# å–æ¨£åƒæ•¸  
sample_step = 200      # å–æ¨£é–“éš”
local_window = 200     # å±€éƒ¨æœ€å¤§å€¼çª—å£

# è¦–è¦ºåŒ–åƒæ•¸
alpha_heatmap = 0.6    # ç†±åŠ›åœ–é€æ˜åº¦
alpha_line = 0.5       # æŠ˜ç·šé€æ˜åº¦
figsize = (16, 8)      # åœ–è¡¨å°ºå¯¸
dpi = 300             # è¼¸å‡ºè§£æåº¦
```

---

## ğŸ“Š **æ•ˆèƒ½åŸºæº–æ¸¬è©¦**

### **è™•ç†æ™‚é–“ (Intel i7-8700K + GTX 1070)**
```
è³‡æ–™å¤§å°: 27,722 æ™‚é–“æ­¥ Ã— 35 ç‰¹å¾µ
çª—å£æ•¸é‡: 545 å€‹çª—å£

éšæ®µæ™‚é–“åˆ†å¸ƒ:
- è³‡æ–™è¼‰å…¥: ~2 ç§’
- çª—å£æå–: ~1 ç§’  
- Grad-CAM è¨ˆç®—: ~45 ç§’ (GPU) / ~180 ç§’ (CPU)
- å¾Œè™•ç†èˆ‡è¦–è¦ºåŒ–: ~3 ç§’
- ç¸½è¨ˆ: ~51 ç§’ (GPU) / ~186 ç§’ (CPU)
```

### **è¨˜æ†¶é«”ä½¿ç”¨é‡**
```
å³°å€¼è¨˜æ†¶é«”: ~4GB (GPU) / ~2GB (RAM)
è¼¸å‡ºæª”æ¡ˆ: ~3MB (PNG) + ~50KB (NPZ)
```

---

*æŠ€è¡“æ–‡æª”ç‰ˆæœ¬: 1.0.0 | æ›´æ–°æ—¥æœŸ: 2025-11-20*